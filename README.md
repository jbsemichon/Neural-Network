# Neural Network
Contains all of the current models of my attempts at implementing a Neural Network that incorporates parts of Stochastic Gradient Descent. None of the programs currently work in their present form.

##Update 21/07/19

Test 1.2 is the younger sibling of Test 1.1 using it as a backbone but diverging from it greatly in the sense that Test 1.2 as far as I can see works in the way a basic neural network is meant to.  Using an example with numbers I found online (https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) I created a neural network that replicates the example - Test 1.3 is set up to only run using the weights and biases given in the example and to train off of them. Test 1.2 is the version that is fully customisable although be weary that I believe that the way the weights are initialise isn't optimal therefore the neural network will not perform well even after training for a large quantity of epochs (eg. 10000). Also I am wondering whether the fact that some of the expected results are 0 is throwing off the network when it tries to backpropagate which is why they have been set to 0.01 instead of 0 and 0.99 rather than 1. The aim of the network in it's current state is to replicate the logic behind an AND logic gate. I can't say that it does this effectively in fact I have found that after many epochs it appears to stop learning altogether and  output the same result for all test cases in the testing phase at the end. Changing the learning rate (lr), number of hidden layers, number of neurons in hidden layers and initial weights and biases would probably all help to increase the accuracy of the network. There is currently no way to save the weights and biases of the network once it has been trained but there is the possibility to test it using your own values inputting them in one at a time for each input neuron.

